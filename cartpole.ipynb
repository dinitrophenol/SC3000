{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions\n",
    "Kent Karsten Pangestu\n",
    "* Task 1,2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "from gym.wrappers import RecordVideo\n",
    "import base64\n",
    "from gym import logger as gymlogger\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space: Discrete(2)\n",
      "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "print(f\"Observation Space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "The action space takes discrete values of either **0** or **1**, pushing the cart to the left or right respectively.\n",
    "\n",
    "### Observation Space\n",
    "The observation space takes continuous values, summarised in the table below.\n",
    "\n",
    "<table style=\"width:70%; font-size: 14px; border: 1px solid black; border-collapse: collapse;\">\n",
    "  <tr>\n",
    "    <th>Num</th>\n",
    "    <th>Observation</th>\n",
    "    <th>Min</th>\n",
    "    <th>Max</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>Cart Position</td>\n",
    "    <td>-4.8</td>\n",
    "    <td>4.8</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Cart Velocity</td>\n",
    "    <td>-Inf</td>\n",
    "    <td>Inf</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Pole Angle</td>\n",
    "    <td>~ -0.418 rad (-24°)</td>\n",
    "    <td>~ 0.418 rad (24°)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>Pole Angular Velocity</td>\n",
    "    <td>-Inf</td>\n",
    "    <td>Inf</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Reward\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of **+1** for every step taken, including the termination step, is allotted.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05).\n",
    "\n",
    "### Terminating Conditions\n",
    "An episode ends if any one of the following occurs:\n",
    "\n",
    "* Pole Angle is greater than ±12°\n",
    "\n",
    "* Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "\n",
    "* Episode length is greater than 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Development of an RL Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the Monte-Carlo: Control algorithm as outlined below.\n",
    "\n",
    "![alt text](mc_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all 4 observed values in the observation space are continuous, we need to discretise them into bins. \n",
    "\n",
    "* `cart_position` is split into **4** bins with intervals of **2.5**\n",
    "* `cart_velocity` is split into **4** bins with intervals of **2.5** \n",
    "* `pole_angle` is split into **10** bins with intervals of **0.1** \n",
    "* `pole_angular_velocity` is split into **40** bins with intervals of **0.25**\n",
    "\n",
    "Through trial-and error, we found that `pole_angular_velocity` is the most important factor in the learning growth, followed by `pole_angle`. Hence, they are set with relatively more bins to account for more precise changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_position_bins = np.linspace(-4.8, 4.8, 4)\n",
    "cart_velocity_bins = np.linspace(-5, 5, 4)\n",
    "pole_angle_bins = np.linspace(-0.5, 0.5, 10)\n",
    "pole_angular_velocity_bins = np.linspace(-5, 5, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determined that exploration is much more important than exploitation in this context due to the large state space. Hence, the hyperparameters are adjusted to accommodate exploration over exploitation in the beginning stages. More specifically,\n",
    "\n",
    "* `discount_factor γ` determines the importance of future rewards when evaluating the expected returns from a state-action pair. It is set at **0.99** to prioritise long-term rewards, encouraging the agent to learn strategies that optimise the overall return rather than focusing only on immediate rewards.\n",
    "\n",
    "* `epsilon ϵ` adds noise to allow the agent to explore other actions besides the current best-known action at a given state. It is set at **1** to encourage complete exploration early on when it has little knowledge of the environment, which is especially important in large state spaces.\n",
    "\n",
    "* `epsilon_decay` decreases ϵ over time. It is set at **0.9995** to gradually transition to exploitation over exploration as the agent gains more knowledge and becomes more confident in its decisions.\n",
    "\n",
    "* `epsilon_min` sets a minimum threshold allowed for ϵ. It is set at **0.07** to ensure that the agent keeps exploring even at the later stages, preventing overly greedy actions that only satisfies a local optima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monte_Carlo():\n",
    "    def __init__(self, env, discount_factor = 0.99, epsilon = 1.0, epsilon_decay = 0.9995, epsilon_min = 0.07) -> None:\n",
    "        self.env = env\n",
    "        self.q_table = defaultdict(int)\n",
    "        self.returns = defaultdict(int)\n",
    "        self.visits = defaultdict(int)\n",
    "        self.discount_factor = discount_factor # γ\n",
    "        self.epsilon = epsilon # ϵ\n",
    "        self.epsilon_decay = epsilon_decay \n",
    "        self.epsilon_min = epsilon_min \n",
    "    \n",
    "    def discretise_state(self, state: np.ndarray) -> tuple:\n",
    "        \"\"\"Puts the state values into the respective discrete bins.\"\"\"\n",
    "        cart_position, cart_velocity, pole_angle, pole_angular_velocity = state\n",
    "        \n",
    "        cart_position = np.digitize(cart_position, cart_position_bins) - 1\n",
    "        cart_velocity = np.digitize(cart_velocity, cart_velocity_bins) - 1\n",
    "        pole_angle = np.digitize(pole_angle, pole_angle_bins) - 1\n",
    "        pole_angular_velocity = np.digitize(pole_angular_velocity, pole_angular_velocity_bins) - 1\n",
    "        return (cart_position, cart_velocity, pole_angle, pole_angular_velocity)\n",
    "    \n",
    "    def decay_epsilon(self) -> None:\n",
    "        \"\"\"Decays epsilon after each episode.\"\"\"\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "    def policy(self, state) -> int:\n",
    "        \"\"\"Rule to determine which action to take at a particular state.\"\"\"\n",
    "        if np.random.rand() < self.epsilon: # Noise added\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return max(range(self.env.action_space.n), key=lambda a: self.q_table.get((state, a), np.random.uniform(low=0, high=1)))\n",
    "\n",
    "    def generate_episode(self) -> tuple:\n",
    "        \"\"\"Generates one round of play.\"\"\"\n",
    "        episode = []\n",
    "        state, _ = self.env.reset() # Environment is reset at the beginning of each episode\n",
    "        state = self.discretise_state(state)\n",
    "        done = False\n",
    "        cumulative_reward = 0\n",
    "        while not done:\n",
    "            action = self.policy(state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = self.discretise_state(next_state)\n",
    "            cumulative_reward += reward\n",
    "            if cumulative_reward > 499:\n",
    "                break\n",
    "        return (episode, cumulative_reward)\n",
    "    \n",
    "    def update_q_table(self, episode) -> None:\n",
    "        \"\"\"Computes returns using first-visit MC method.\"\"\"\n",
    "        G = 0\n",
    "        state_action_visited = defaultdict(int)\n",
    "\n",
    "        # Iterate backwards according to G(t) = r(t) + y * G(t+1)\n",
    "        for i in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[i]\n",
    "            G = reward + self.discount_factor * G\n",
    "            state_action_visited[(state, action)] = G\n",
    "\n",
    "            for key, val in state_action_visited.items():\n",
    "                # Update Q table for each state-action pair\n",
    "                self.returns[key] += val\n",
    "                self.visits[key] += 1\n",
    "                self.q_table[key] = self.returns[key] / self.visits[key]\n",
    "    \n",
    "    def train_episodes(self, num_episodes) -> None:\n",
    "        reward = 0 \n",
    "        for i in range(1,num_episodes+2):\n",
    "            episode , cumulative_reward = self.generate_episode()\n",
    "            self.update_q_table(episode)\n",
    "            reward += cumulative_reward\n",
    "            self.decay_epsilon()   \n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Mean reward for the first {i} episodes: {reward/1000}\")\n",
    "                reward = 0\n",
    "        \n",
    "    def run(self, raw_state) -> int:\n",
    "        \"\"\"Determines the optimal action to take based on current knowledge.\"\"\"\n",
    "        state = self.discretise_state(raw_state)\n",
    "        return max(range(self.env.action_space.n), key=lambda a: self.q_table.get(state, a))\n",
    "    \n",
    "    def test_episodes(self, num_episodes) -> list:\n",
    "        reward_list = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.run(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                cumulative_reward += reward\n",
    "                if cumulative_reward > 499:\n",
    "                    break\n",
    "                state = next_state\n",
    "                \n",
    "            reward_list.append(cumulative_reward)\n",
    "            \n",
    "        return reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is trained on 6000 episodes, and the average reward every 1000 episodes is indicated to highlight the agent's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for the first 1000 episodes: 46.131\n",
      "Mean reward for the first 2000 episodes: 142.257\n",
      "Mean reward for the first 3000 episodes: 199.851\n",
      "Mean reward for the first 4000 episodes: 195.692\n",
      "Mean reward for the first 5000 episodes: 190.172\n",
      "Mean reward for the first 6000 episodes: 189.354\n"
     ]
    }
   ],
   "source": [
    "Monte_Carlo_Agent = Monte_Carlo(env)\n",
    "Monte_Carlo_Agent.train_episodes(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implemented the Q-Learning algorithm as outlined below. We found that Q-Learning is a much more effective algorithm, given the same hyperparameters. \n",
    "\n",
    "<img src=\"q_learning_algorithm.png\" width=\"800\" height=\"340\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored the algorithm given the same number of bins for each variable, the same values for `discount_factor γ` at 0.99, `epsilon ϵ` at 1, `epsilon_decay` at 0.995, and `epsilon_min` at 0.07 as with the previous algorithm. In addition, we added `learning_rate α`, which is needed in a Q-Learning algorithm.\n",
    "\n",
    "* `learning_rate α` controls how much the Q-values are updated in each step of an episode. It is set low at **0.1** to allow for more stability during the training process by ensuring that the agent learns more gradually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.9995, epsilon_min=0.07):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.observation = [4, 4, 10, 40] # Number of bins set for each variable in an observation\n",
    "        self.window_size = np.array([2.5, 2.5, 0.1, 0.25]) # Size of each bin\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(self.observation + [env.action_space.n])) # Q-table initialised with arbitrary values\n",
    "\n",
    "    def discretise_state(self, state):\n",
    "        discrete_state = state/self.window_size + np.array([2,2,5,20]) # Normalise the state values to ensure they are within bounds of the bin \n",
    "        return tuple(discrete_state.astype(int)) # Truncate and convert to tuple to use as a key in the Q-table\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    def policy(self, state):\n",
    "        if np.random.random() < self.epsilon: # Noise added\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state]) \n",
    "        \n",
    "    def generate_episode(self):\n",
    "        state, _ = self.env.reset()\n",
    "        state = self.discretise_state(state)\n",
    "        done = False\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.policy(state)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            next_state = self.discretise_state(next_state)\n",
    "            cumulative_reward += reward\n",
    "\n",
    "            if cumulative_reward > 499:\n",
    "                break\n",
    "            \n",
    "            self.update_q_table(state, next_state, action, reward)\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        return cumulative_reward\n",
    "\n",
    "    def update_q_table(self, state, next_state, action, reward):\n",
    "        current_q = self.q_table[state + (action,)]\n",
    "        next_max_q = np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Update Q-value using the Bellman equation: Q(s, a) = (1 - α) * Q(s, a) + α * (r + γ * max_a(Q(s', a')))\n",
    "        new_q = (1 - self.learning_rate) * current_q + self.learning_rate * (reward + self.discount_factor * next_max_q)\n",
    "        self.q_table[state + (action,)] = new_q\n",
    "\n",
    "    def train_episodes(self, num_episodes):\n",
    "        reward = 0 \n",
    "        for episode in range(1,num_episodes+2):\n",
    "            reward += self.generate_episode()\n",
    "            self.decay_epsilon()   \n",
    "            if episode % 1000 == 0:\n",
    "                print(f\"Mean reward for the first {episode} episodes: {reward/1000}\")\n",
    "                reward = 0\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def run(self, raw_state):\n",
    "        \"\"\"Determines the optimal action to take based on current knowledge.\"\"\"\n",
    "        state = self.discretise_state(raw_state)\n",
    "        action = np.argmax(self.q_table[state])\n",
    "        return action\n",
    "    \n",
    "    def test_episodes(self, num_episodes):\n",
    "        reward_list = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.run(state)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                cumulative_reward += reward\n",
    "                if cumulative_reward > 499:\n",
    "                    break\n",
    "                state = next_state\n",
    "                \n",
    "            reward_list.append(cumulative_reward)\n",
    "            \n",
    "        return reward_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is trained on 5000 episodes, and the average reward every 1000 episodes is indicated to highlight the agent's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward for the first 1000 episodes: 32.321\n",
      "Mean reward for the first 2000 episodes: 148.846\n",
      "Mean reward for the first 3000 episodes: 314.279\n",
      "Mean reward for the first 4000 episodes: 401.773\n",
      "Mean reward for the first 5000 episodes: 414.346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_agent = QLearning(env)\n",
    "Q_agent.train_episodes(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the agent learns much better with Q-Learning than Monte-Carlo. \n",
    "\n",
    "### Hence, we will be using the Q-Learning agent for the remaining tasks.\n",
    "\n",
    "The agent is tested on a random state of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-4.6075437e-02 -4.1246563e-02  6.3011728e-05  3.4020457e-02]\n",
      "Chosen action: 1\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "action = Q_agent.run(state)\n",
    "print(\"Observation:\", state)\n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Demonstrate the effectiveness of the RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative reward: 500.0\n",
      "Is my agent good enough? True\n"
     ]
    }
   ],
   "source": [
    "episode_results = Q_agent.test_episodes(100)\n",
    "\n",
    "plt.plot(episode_results)\n",
    "plt.title(\"Cumulative reward for each episode\")\n",
    "plt.ylabel('Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average cumulative reward:\", np.mean(episode_results))\n",
    "print(\"Is my agent good enough?\", np.mean(episode_results) > 195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Render one episode played by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video d:\\Files\\VS Code\\SC3000\\SC3000\\video\\rl-video-episode-0.mp4.\n",
      "MoviePy - Writing video d:\\Files\\VS Code\\SC3000\\SC3000\\video\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready d:\\Files\\VS Code\\SC3000\\SC3000\\video\\rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "def show_video():\n",
    "    \"\"\"Finds and displays the recorded video.\"\"\"\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'rb').read()\n",
    "        encoded = base64.b64encode(video).decode('ascii')\n",
    "        display(HTML(f'<video width=\"600\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\"></video>'))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "\n",
    "def videoEpisode(agent):\n",
    "    \"\"\"Runs one episode, recording the video.\"\"\"\n",
    "    env = RecordVideo(gym.make('CartPole-v1', render_mode='rgb_array'), \"./video\")\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.run(state)  # Assuming `agent.run(state)` chooses an action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "    \n",
    "    env.close()\n",
    "    show_video()\n",
    "\n",
    "    return\n",
    "\n",
    "# Call videoEpisode and pass your agent\n",
    "videoEpisode(Q_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
