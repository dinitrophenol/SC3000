{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Initial observations: (array([-0.01628398, -0.0382522 , -0.04780297, -0.04398627], dtype=float32), {})\n",
      "New observations after choosing action 0: [-0.01704903 -0.23265724 -0.04868269  0.23323946]\n",
      "Reward for this step: 1.0\n",
      "Is this round done? False\n",
      "Cumulative reward for this round: 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kentp\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Initial observations:\", observation)\n",
    "\n",
    "observation, reward, done, info, extra_info = env.step(0)\n",
    "\n",
    "print(\"New observations after choosing action 0:\", observation)\n",
    "print(\"Reward for this step:\", reward)\n",
    "print(\"Is this round done?\", done)\n",
    "\n",
    "observation = env.reset()\n",
    "cumulative_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    observation, reward, done, info, extra_info = env.step(0)\n",
    "    cumulative_reward += reward\n",
    "print(\"Cumulative reward for this round:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:  Development of an RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Monte_Carlo():\n",
    "    def __init__(self, env) -> None:\n",
    "        self.env = env\n",
    "        self.q_table = defaultdict(int)\n",
    "        self.r_table = defaultdict(int)\n",
    "        self.discount_factor = 0.9 # test\n",
    "        self.epsilon = 0.1 # test \n",
    "    \n",
    "    def discretise_state(self, state: np.ndarray) -> np.ndarray:\n",
    "        state[0] = round(state[0], 1)\n",
    "        state[1] = round(state[1], 0)\n",
    "        state[2] = round(state[2], 1)\n",
    "        state[3] = round(state[3], 0)\n",
    "        return state\n",
    "    \n",
    "    def generate_episode(self, policy) -> list:\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, info_1, info_2 = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def generate_return(self, episode) -> None:\n",
    "        G = defaultdict(int)\n",
    "\n",
    "        accumulated_reward = 0\n",
    "\n",
    "        # Iterate backwards according to G(t) = r(t) + y * G(t+1)\n",
    "        for i in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[i]\n",
    "            accumulated_reward = reward + self.discount_factor * accumulated_reward\n",
    "            G[(state, action)] += accumulated_reward\n",
    "        \n",
    "        for (state, action) in G:\n",
    "            self.r_table[(state, action)].append(G[(state, action)])\n",
    "\n",
    "        return\n",
    "    \n",
    "    def update_q_table(self) -> None:\n",
    "        for (state, action) in self.r_table:\n",
    "            self.q_table[(state, action)] = np.mean(self.r_table[(state, action)])\n",
    "\n",
    "    def policy(self, state) -> int:\n",
    "        q_values = [self.q_table.get((state, a), 0) for a in range(self.env.action_space.n)]\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(range(self.env.action_space.n))\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
